{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1-a: Deep Learning Hello World! (Baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective: To be able to implement a basic MLP in Keras for MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Taking care of the imports which includes numpy, datasets, models, layers, optimizers, and utils. <br />\n",
    "You will also be able to tell if your set-up is correct/complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.models import model_from_json\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Set-up some constants to be utilized in the training/testing of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_EPOCH = 200\n",
    "BATCH_SIZE = 128\n",
    "VERBOSE = 1\n",
    "NB_CLASSES = 10   # number of outputs = number of digits, i.e. 0,1,2,3,4,5,6,7,8,9\n",
    "OPTIMIZER = SGD() # Stocastic Gradient Descent optimizer\n",
    "N_HIDDEN = 128\n",
    "VALIDATION_SPLIT=0.2 # how much TRAIN dataset is reserved for VALIDATION\n",
    "\n",
    "np.random.seed(1983)  # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Load the MNIST Dataset which are shuffled and split between train and test sets <br\\>\n",
    "- X_train is 60000 rows of 28x28 values\n",
    "- X_test is 10000 rows of 28x28 values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x153eb8a6588>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAABrCAYAAABnlHmpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nnyAVuiAIgk+QCl0QBMEn/D97OBSNw5IoIAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15382067f60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.imshow(X_train[0])        # first train\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.imshow(X_train[59999])    # last train\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.imshow(X_test[0])         # first test \n",
    "plt.subplot(1, 4, 4)\n",
    "plt.imshow(X_test[9999])      # last test  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Preprocess the input data by reshaping it, converting it to float, and normalizing it [0-1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) train samples\n",
      "(10000, 784) test samples\n"
     ]
    }
   ],
   "source": [
    "# reshape\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# normalize \n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "print(X_train.shape, 'train samples')\n",
    "print(X_test.shape, 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Convert class vectors to binary class matrices; One-Hot-Encoding (OHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Create the model: Input:784 ==> Output:10 (with Softmax activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 10)                7850      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 7,850\n",
      "Trainable params: 7,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(NB_CLASSES, input_shape=(784,)))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 7: Compile the model with categorical_crossentropy loss function, SGD optimizer, and accuracy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 8: Perform the training with 128 batch size, 200 epochs, and 20 % of the train data used for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/200\n",
      "48000/48000 [==============================] - 1s - loss: 1.4054 - acc: 0.6486 - val_loss: 0.9016 - val_acc: 0.8237\n",
      "Epoch 2/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.7984 - acc: 0.8248 - val_loss: 0.6587 - val_acc: 0.8559\n",
      "Epoch 3/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.6451 - acc: 0.8484 - val_loss: 0.5609 - val_acc: 0.8706\n",
      "Epoch 4/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.5713 - acc: 0.8606 - val_loss: 0.5073 - val_acc: 0.8788\n",
      "Epoch 5/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.5265 - acc: 0.8677 - val_loss: 0.4729 - val_acc: 0.8836\n",
      "Epoch 6/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.4958 - acc: 0.8736 - val_loss: 0.4488 - val_acc: 0.8872\n",
      "Epoch 7/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.4733 - acc: 0.8779 - val_loss: 0.4305 - val_acc: 0.8902\n",
      "Epoch 8/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.4557 - acc: 0.8812 - val_loss: 0.4166 - val_acc: 0.8918\n",
      "Epoch 9/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.4417 - acc: 0.8840 - val_loss: 0.4049 - val_acc: 0.8938\n",
      "Epoch 10/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.4301 - acc: 0.8862 - val_loss: 0.3954 - val_acc: 0.8950\n",
      "Epoch 11/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.4203 - acc: 0.8885 - val_loss: 0.3873 - val_acc: 0.89720.88\n",
      "Epoch 12/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.4119 - acc: 0.8899 - val_loss: 0.3804 - val_acc: 0.8987\n",
      "Epoch 13/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.4045 - acc: 0.8913 - val_loss: 0.3743 - val_acc: 0.9004\n",
      "Epoch 14/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3980 - acc: 0.8929 - val_loss: 0.3692 - val_acc: 0.9008\n",
      "Epoch 15/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3923 - acc: 0.8937 - val_loss: 0.3645 - val_acc: 0.9023\n",
      "Epoch 16/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3872 - acc: 0.8949 - val_loss: 0.3601 - val_acc: 0.9046\n",
      "Epoch 17/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3824 - acc: 0.8959 - val_loss: 0.3563 - val_acc: 0.9040\n",
      "Epoch 18/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3782 - acc: 0.8963 - val_loss: 0.3528 - val_acc: 0.9048\n",
      "Epoch 19/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3743 - acc: 0.8976 - val_loss: 0.3496 - val_acc: 0.9053\n",
      "Epoch 20/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3707 - acc: 0.8982 - val_loss: 0.3465 - val_acc: 0.9058\n",
      "Epoch 21/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3674 - acc: 0.8990 - val_loss: 0.3441 - val_acc: 0.9065\n",
      "Epoch 22/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3643 - acc: 0.8995 - val_loss: 0.3415 - val_acc: 0.9072\n",
      "Epoch 23/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3614 - acc: 0.8999 - val_loss: 0.3390 - val_acc: 0.9081\n",
      "Epoch 24/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3587 - acc: 0.9005 - val_loss: 0.3369 - val_acc: 0.9086\n",
      "Epoch 25/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3562 - acc: 0.9015 - val_loss: 0.3349 - val_acc: 0.9085\n",
      "Epoch 26/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3539 - acc: 0.9020 - val_loss: 0.3330 - val_acc: 0.9087\n",
      "Epoch 27/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3516 - acc: 0.9026 - val_loss: 0.3310 - val_acc: 0.9088\n",
      "Epoch 28/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3495 - acc: 0.9032 - val_loss: 0.3294 - val_acc: 0.9091\n",
      "Epoch 29/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3475 - acc: 0.9037 - val_loss: 0.3278 - val_acc: 0.9092\n",
      "Epoch 30/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3456 - acc: 0.9044 - val_loss: 0.3263 - val_acc: 0.9097\n",
      "Epoch 31/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3438 - acc: 0.9047 - val_loss: 0.3248 - val_acc: 0.9098\n",
      "Epoch 32/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3421 - acc: 0.9052 - val_loss: 0.3234 - val_acc: 0.9103\n",
      "Epoch 33/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3405 - acc: 0.9054 - val_loss: 0.3221 - val_acc: 0.9105\n",
      "Epoch 34/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3388 - acc: 0.9059 - val_loss: 0.3208 - val_acc: 0.9107\n",
      "Epoch 35/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3374 - acc: 0.9065 - val_loss: 0.3197 - val_acc: 0.9112\n",
      "Epoch 36/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3359 - acc: 0.9069 - val_loss: 0.3185 - val_acc: 0.9115\n",
      "Epoch 37/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3346 - acc: 0.9071 - val_loss: 0.3174 - val_acc: 0.9115\n",
      "Epoch 38/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3332 - acc: 0.9073 - val_loss: 0.3164 - val_acc: 0.9118\n",
      "Epoch 39/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3319 - acc: 0.9078 - val_loss: 0.3153 - val_acc: 0.9123\n",
      "Epoch 40/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3307 - acc: 0.9079 - val_loss: 0.3144 - val_acc: 0.9123\n",
      "Epoch 41/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3295 - acc: 0.9085 - val_loss: 0.3135 - val_acc: 0.9125\n",
      "Epoch 42/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3284 - acc: 0.9086 - val_loss: 0.3125 - val_acc: 0.9128\n",
      "Epoch 43/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3273 - acc: 0.9087 - val_loss: 0.3117 - val_acc: 0.9127\n",
      "Epoch 44/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3262 - acc: 0.9093 - val_loss: 0.3109 - val_acc: 0.9128\n",
      "Epoch 45/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3252 - acc: 0.9094 - val_loss: 0.3100 - val_acc: 0.9131\n",
      "Epoch 46/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3242 - acc: 0.9098 - val_loss: 0.3092 - val_acc: 0.9139\n",
      "Epoch 47/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3233 - acc: 0.9097 - val_loss: 0.3086 - val_acc: 0.9132\n",
      "Epoch 48/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3223 - acc: 0.9101 - val_loss: 0.3078 - val_acc: 0.9144\n",
      "Epoch 49/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3214 - acc: 0.9108 - val_loss: 0.3072 - val_acc: 0.9137\n",
      "Epoch 50/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3206 - acc: 0.9107 - val_loss: 0.3064 - val_acc: 0.9139\n",
      "Epoch 51/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3197 - acc: 0.9109 - val_loss: 0.3059 - val_acc: 0.9141\n",
      "Epoch 52/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3189 - acc: 0.9112 - val_loss: 0.3052 - val_acc: 0.9147\n",
      "Epoch 53/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3181 - acc: 0.9114 - val_loss: 0.3045 - val_acc: 0.9148\n",
      "Epoch 54/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3173 - acc: 0.9117 - val_loss: 0.3039 - val_acc: 0.9148\n",
      "Epoch 55/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3165 - acc: 0.9120 - val_loss: 0.3033 - val_acc: 0.9153\n",
      "Epoch 56/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3158 - acc: 0.9122 - val_loss: 0.3027 - val_acc: 0.9147\n",
      "Epoch 57/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3150 - acc: 0.9124 - val_loss: 0.3023 - val_acc: 0.9159\n",
      "Epoch 58/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3143 - acc: 0.9125 - val_loss: 0.3018 - val_acc: 0.9152\n",
      "Epoch 59/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3137 - acc: 0.9126 - val_loss: 0.3011 - val_acc: 0.9155\n",
      "Epoch 60/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3130 - acc: 0.9131 - val_loss: 0.3006 - val_acc: 0.9157\n",
      "Epoch 61/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3124 - acc: 0.9135 - val_loss: 0.3001 - val_acc: 0.9161\n",
      "Epoch 62/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3117 - acc: 0.9135 - val_loss: 0.2997 - val_acc: 0.9157\n",
      "Epoch 63/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3111 - acc: 0.9134 - val_loss: 0.2992 - val_acc: 0.9163\n",
      "Epoch 64/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 1s - loss: 0.3105 - acc: 0.9139 - val_loss: 0.2988 - val_acc: 0.9167\n",
      "Epoch 65/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3098 - acc: 0.9140 - val_loss: 0.2983 - val_acc: 0.9162\n",
      "Epoch 66/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3093 - acc: 0.9140 - val_loss: 0.2978 - val_acc: 0.9171\n",
      "Epoch 67/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3088 - acc: 0.9143 - val_loss: 0.2974 - val_acc: 0.9169\n",
      "Epoch 68/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3082 - acc: 0.9141 - val_loss: 0.2970 - val_acc: 0.9172\n",
      "Epoch 69/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3076 - acc: 0.9144 - val_loss: 0.2967 - val_acc: 0.9172\n",
      "Epoch 70/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3071 - acc: 0.9148 - val_loss: 0.2963 - val_acc: 0.9176\n",
      "Epoch 71/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3066 - acc: 0.9147 - val_loss: 0.2958 - val_acc: 0.9170\n",
      "Epoch 72/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3060 - acc: 0.9149 - val_loss: 0.2955 - val_acc: 0.9177\n",
      "Epoch 73/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3056 - acc: 0.9148 - val_loss: 0.2951 - val_acc: 0.9175\n",
      "Epoch 74/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3051 - acc: 0.9150 - val_loss: 0.2947 - val_acc: 0.9175\n",
      "Epoch 75/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3046 - acc: 0.9154 - val_loss: 0.2943 - val_acc: 0.9175\n",
      "Epoch 76/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3041 - acc: 0.9154 - val_loss: 0.2939 - val_acc: 0.9180\n",
      "Epoch 77/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3037 - acc: 0.9157 - val_loss: 0.2936 - val_acc: 0.9182\n",
      "Epoch 78/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3032 - acc: 0.9156 - val_loss: 0.2933 - val_acc: 0.9182\n",
      "Epoch 79/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3027 - acc: 0.9160 - val_loss: 0.2930 - val_acc: 0.9184\n",
      "Epoch 80/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3023 - acc: 0.9160 - val_loss: 0.2926 - val_acc: 0.9184\n",
      "Epoch 81/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3019 - acc: 0.9163 - val_loss: 0.2924 - val_acc: 0.9184\n",
      "Epoch 82/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3014 - acc: 0.9160 - val_loss: 0.2920 - val_acc: 0.9187\n",
      "Epoch 83/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3010 - acc: 0.9162 - val_loss: 0.2917 - val_acc: 0.9188\n",
      "Epoch 84/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3006 - acc: 0.9166 - val_loss: 0.2915 - val_acc: 0.9184\n",
      "Epoch 85/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.3002 - acc: 0.9165 - val_loss: 0.2912 - val_acc: 0.9185\n",
      "Epoch 86/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2999 - acc: 0.9167 - val_loss: 0.2909 - val_acc: 0.9186\n",
      "Epoch 87/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2994 - acc: 0.9165 - val_loss: 0.2906 - val_acc: 0.9188\n",
      "Epoch 88/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2990 - acc: 0.9165 - val_loss: 0.2904 - val_acc: 0.9188\n",
      "Epoch 89/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2987 - acc: 0.9169 - val_loss: 0.2900 - val_acc: 0.9187\n",
      "Epoch 90/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2983 - acc: 0.9173 - val_loss: 0.2898 - val_acc: 0.9192\n",
      "Epoch 91/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2979 - acc: 0.9170 - val_loss: 0.2895 - val_acc: 0.9192\n",
      "Epoch 92/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2976 - acc: 0.9172 - val_loss: 0.2893 - val_acc: 0.9192\n",
      "Epoch 93/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2973 - acc: 0.9173 - val_loss: 0.2890 - val_acc: 0.9192\n",
      "Epoch 94/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2968 - acc: 0.9177 - val_loss: 0.2889 - val_acc: 0.9193\n",
      "Epoch 95/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2965 - acc: 0.9173 - val_loss: 0.2886 - val_acc: 0.9198\n",
      "Epoch 96/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2962 - acc: 0.9175 - val_loss: 0.2883 - val_acc: 0.9194\n",
      "Epoch 97/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2959 - acc: 0.9178 - val_loss: 0.2880 - val_acc: 0.9195\n",
      "Epoch 98/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2956 - acc: 0.9179 - val_loss: 0.2878 - val_acc: 0.9196\n",
      "Epoch 99/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2952 - acc: 0.9178 - val_loss: 0.2877 - val_acc: 0.9201\n",
      "Epoch 100/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2949 - acc: 0.9179 - val_loss: 0.2874 - val_acc: 0.92080.917\n",
      "Epoch 101/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2946 - acc: 0.9180 - val_loss: 0.2871 - val_acc: 0.9200\n",
      "Epoch 102/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2943 - acc: 0.9179 - val_loss: 0.2869 - val_acc: 0.9199\n",
      "Epoch 103/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2939 - acc: 0.9180 - val_loss: 0.2868 - val_acc: 0.9199\n",
      "Epoch 104/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2937 - acc: 0.9181 - val_loss: 0.2866 - val_acc: 0.9202\n",
      "Epoch 105/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2934 - acc: 0.9181 - val_loss: 0.2863 - val_acc: 0.9198\n",
      "Epoch 106/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2931 - acc: 0.9186 - val_loss: 0.2862 - val_acc: 0.9201\n",
      "Epoch 107/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2928 - acc: 0.9186 - val_loss: 0.2859 - val_acc: 0.9203\n",
      "Epoch 108/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2925 - acc: 0.9183 - val_loss: 0.2856 - val_acc: 0.9204\n",
      "Epoch 109/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2922 - acc: 0.9187 - val_loss: 0.2855 - val_acc: 0.9202\n",
      "Epoch 110/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2920 - acc: 0.9188 - val_loss: 0.2853 - val_acc: 0.9202\n",
      "Epoch 111/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2917 - acc: 0.9185 - val_loss: 0.2852 - val_acc: 0.9203\n",
      "Epoch 112/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2914 - acc: 0.9187 - val_loss: 0.2849 - val_acc: 0.9201\n",
      "Epoch 113/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2912 - acc: 0.9191 - val_loss: 0.2847 - val_acc: 0.9203\n",
      "Epoch 114/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2909 - acc: 0.9190 - val_loss: 0.2845 - val_acc: 0.9205\n",
      "Epoch 115/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2906 - acc: 0.9189 - val_loss: 0.2844 - val_acc: 0.9207\n",
      "Epoch 116/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2904 - acc: 0.9190 - val_loss: 0.2842 - val_acc: 0.9204\n",
      "Epoch 117/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2901 - acc: 0.9191 - val_loss: 0.2840 - val_acc: 0.9211\n",
      "Epoch 118/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2898 - acc: 0.9192 - val_loss: 0.2840 - val_acc: 0.9208\n",
      "Epoch 119/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2896 - acc: 0.9193 - val_loss: 0.2838 - val_acc: 0.9207\n",
      "Epoch 120/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2894 - acc: 0.9193 - val_loss: 0.2835 - val_acc: 0.9210\n",
      "Epoch 121/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2891 - acc: 0.9192 - val_loss: 0.2833 - val_acc: 0.9209\n",
      "Epoch 122/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2889 - acc: 0.9196 - val_loss: 0.2832 - val_acc: 0.9212\n",
      "Epoch 123/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2886 - acc: 0.9195 - val_loss: 0.2831 - val_acc: 0.9208\n",
      "Epoch 124/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2884 - acc: 0.9194 - val_loss: 0.2829 - val_acc: 0.9210\n",
      "Epoch 125/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2882 - acc: 0.9196 - val_loss: 0.2827 - val_acc: 0.9212\n",
      "Epoch 126/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2880 - acc: 0.9196 - val_loss: 0.2826 - val_acc: 0.9210\n",
      "Epoch 127/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 1s - loss: 0.2877 - acc: 0.9198 - val_loss: 0.2825 - val_acc: 0.9216\n",
      "Epoch 128/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2875 - acc: 0.9199 - val_loss: 0.2823 - val_acc: 0.9212\n",
      "Epoch 129/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2873 - acc: 0.9199 - val_loss: 0.2822 - val_acc: 0.9212\n",
      "Epoch 130/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2871 - acc: 0.9201 - val_loss: 0.2820 - val_acc: 0.9216\n",
      "Epoch 131/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2868 - acc: 0.9202 - val_loss: 0.2819 - val_acc: 0.9214\n",
      "Epoch 132/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2867 - acc: 0.9200 - val_loss: 0.2817 - val_acc: 0.9216\n",
      "Epoch 133/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2864 - acc: 0.9199 - val_loss: 0.2815 - val_acc: 0.9217\n",
      "Epoch 134/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2862 - acc: 0.9202 - val_loss: 0.2814 - val_acc: 0.9213\n",
      "Epoch 135/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2860 - acc: 0.9202 - val_loss: 0.2813 - val_acc: 0.9216\n",
      "Epoch 136/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2858 - acc: 0.9204 - val_loss: 0.2812 - val_acc: 0.9214\n",
      "Epoch 137/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2856 - acc: 0.9203 - val_loss: 0.2811 - val_acc: 0.9216\n",
      "Epoch 138/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2854 - acc: 0.9202 - val_loss: 0.2809 - val_acc: 0.9216\n",
      "Epoch 139/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2852 - acc: 0.9205 - val_loss: 0.2808 - val_acc: 0.9218\n",
      "Epoch 140/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2850 - acc: 0.9206 - val_loss: 0.2807 - val_acc: 0.9217\n",
      "Epoch 141/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2848 - acc: 0.9208 - val_loss: 0.2806 - val_acc: 0.9217\n",
      "Epoch 142/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2846 - acc: 0.9206 - val_loss: 0.2804 - val_acc: 0.9220\n",
      "Epoch 143/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2844 - acc: 0.9203 - val_loss: 0.2802 - val_acc: 0.9219\n",
      "Epoch 144/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2842 - acc: 0.9208 - val_loss: 0.2801 - val_acc: 0.9222\n",
      "Epoch 145/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2841 - acc: 0.9207 - val_loss: 0.2800 - val_acc: 0.9221\n",
      "Epoch 146/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2839 - acc: 0.9209 - val_loss: 0.2799 - val_acc: 0.9222\n",
      "Epoch 147/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2837 - acc: 0.9209 - val_loss: 0.2798 - val_acc: 0.9221\n",
      "Epoch 148/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2835 - acc: 0.9209 - val_loss: 0.2797 - val_acc: 0.9225\n",
      "Epoch 149/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2833 - acc: 0.9209 - val_loss: 0.2796 - val_acc: 0.9222\n",
      "Epoch 150/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2831 - acc: 0.9212 - val_loss: 0.2795 - val_acc: 0.9220\n",
      "Epoch 151/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2830 - acc: 0.9210 - val_loss: 0.2793 - val_acc: 0.9219\n",
      "Epoch 152/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2828 - acc: 0.9208 - val_loss: 0.2793 - val_acc: 0.9222\n",
      "Epoch 153/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2826 - acc: 0.9213 - val_loss: 0.2792 - val_acc: 0.9222\n",
      "Epoch 154/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2824 - acc: 0.9212 - val_loss: 0.2790 - val_acc: 0.9224\n",
      "Epoch 155/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2823 - acc: 0.9213 - val_loss: 0.2789 - val_acc: 0.9226\n",
      "Epoch 156/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2821 - acc: 0.9214 - val_loss: 0.2787 - val_acc: 0.9224\n",
      "Epoch 157/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2819 - acc: 0.9216 - val_loss: 0.2787 - val_acc: 0.9223\n",
      "Epoch 158/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2818 - acc: 0.9214 - val_loss: 0.2786 - val_acc: 0.9223\n",
      "Epoch 159/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2816 - acc: 0.9216 - val_loss: 0.2785 - val_acc: 0.9225\n",
      "Epoch 160/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2815 - acc: 0.9217 - val_loss: 0.2784 - val_acc: 0.9223\n",
      "Epoch 161/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2813 - acc: 0.9217 - val_loss: 0.2783 - val_acc: 0.9224\n",
      "Epoch 162/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2811 - acc: 0.9219 - val_loss: 0.2782 - val_acc: 0.9224\n",
      "Epoch 163/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2809 - acc: 0.9215 - val_loss: 0.2781 - val_acc: 0.9226\n",
      "Epoch 164/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2808 - acc: 0.9218 - val_loss: 0.2780 - val_acc: 0.9227\n",
      "Epoch 165/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2806 - acc: 0.9218 - val_loss: 0.2780 - val_acc: 0.9224\n",
      "Epoch 166/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2805 - acc: 0.9218 - val_loss: 0.2777 - val_acc: 0.9228\n",
      "Epoch 167/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2803 - acc: 0.9219 - val_loss: 0.2777 - val_acc: 0.9227\n",
      "Epoch 168/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2801 - acc: 0.9219 - val_loss: 0.2777 - val_acc: 0.9223\n",
      "Epoch 169/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2800 - acc: 0.9223 - val_loss: 0.2775 - val_acc: 0.9228\n",
      "Epoch 170/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2799 - acc: 0.9221 - val_loss: 0.2774 - val_acc: 0.9227\n",
      "Epoch 171/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2797 - acc: 0.9220 - val_loss: 0.2773 - val_acc: 0.9232\n",
      "Epoch 172/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2796 - acc: 0.9220 - val_loss: 0.2773 - val_acc: 0.9230\n",
      "Epoch 173/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2794 - acc: 0.9221 - val_loss: 0.2772 - val_acc: 0.9227\n",
      "Epoch 174/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2793 - acc: 0.9221 - val_loss: 0.2771 - val_acc: 0.9229\n",
      "Epoch 175/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2791 - acc: 0.9225 - val_loss: 0.2770 - val_acc: 0.9232\n",
      "Epoch 176/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2790 - acc: 0.9225 - val_loss: 0.2769 - val_acc: 0.9232\n",
      "Epoch 177/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2788 - acc: 0.9222 - val_loss: 0.2768 - val_acc: 0.9227\n",
      "Epoch 178/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2787 - acc: 0.9225 - val_loss: 0.2767 - val_acc: 0.9229\n",
      "Epoch 179/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2786 - acc: 0.9224 - val_loss: 0.2767 - val_acc: 0.9229\n",
      "Epoch 180/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2784 - acc: 0.9226 - val_loss: 0.2766 - val_acc: 0.9230\n",
      "Epoch 181/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2783 - acc: 0.9228 - val_loss: 0.2765 - val_acc: 0.9232\n",
      "Epoch 182/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2781 - acc: 0.9226 - val_loss: 0.2764 - val_acc: 0.9232\n",
      "Epoch 183/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2780 - acc: 0.9227 - val_loss: 0.2764 - val_acc: 0.9230\n",
      "Epoch 184/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2779 - acc: 0.9227 - val_loss: 0.2762 - val_acc: 0.9231\n",
      "Epoch 185/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2777 - acc: 0.9229 - val_loss: 0.2762 - val_acc: 0.9230\n",
      "Epoch 186/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2776 - acc: 0.9229 - val_loss: 0.2761 - val_acc: 0.9233\n",
      "Epoch 187/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2775 - acc: 0.9228 - val_loss: 0.2760 - val_acc: 0.9232\n",
      "Epoch 188/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2774 - acc: 0.9229 - val_loss: 0.2759 - val_acc: 0.9233\n",
      "Epoch 189/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2772 - acc: 0.9229 - val_loss: 0.2758 - val_acc: 0.9232\n",
      "Epoch 190/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48000/48000 [==============================] - 1s - loss: 0.2771 - acc: 0.9230 - val_loss: 0.2758 - val_acc: 0.9232\n",
      "Epoch 191/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2770 - acc: 0.9230 - val_loss: 0.2757 - val_acc: 0.9232\n",
      "Epoch 192/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2768 - acc: 0.9229 - val_loss: 0.2756 - val_acc: 0.9236\n",
      "Epoch 193/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2767 - acc: 0.9230 - val_loss: 0.2756 - val_acc: 0.9234\n",
      "Epoch 194/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2766 - acc: 0.9229 - val_loss: 0.2755 - val_acc: 0.9234\n",
      "Epoch 195/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2765 - acc: 0.9231 - val_loss: 0.2754 - val_acc: 0.9237\n",
      "Epoch 196/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2764 - acc: 0.9232 - val_loss: 0.2754 - val_acc: 0.9237\n",
      "Epoch 197/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2762 - acc: 0.9230 - val_loss: 0.2753 - val_acc: 0.9237\n",
      "Epoch 198/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2761 - acc: 0.9231 - val_loss: 0.2752 - val_acc: 0.9242\n",
      "Epoch 199/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2760 - acc: 0.9233 - val_loss: 0.2751 - val_acc: 0.9238\n",
      "Epoch 200/200\n",
      "48000/48000 [==============================] - 1s - loss: 0.2758 - acc: 0.9231 - val_loss: 0.2751 - val_acc: 0.9237\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=BATCH_SIZE, epochs=NB_EPOCH,\n",
    "                    verbose=VERBOSE, validation_split=VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 9: Evaluate the model on the test dataset (10,000 images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9856/10000 [============================>.] - ETA: 0s\n",
      "Test score: 0.276437405369\n",
      "Test accuracy: 0.9228\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"\\nTest score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Optional] Step 10: Save the model (serialized) to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows\n",
      " Volume Serial Number is 7252-C405\n",
      "\n",
      " Directory of C:\\Users\\cobalt\\workspace\n",
      "\n",
      "09/17/2017  12:53 PM    <DIR>          .\n",
      "09/17/2017  12:53 PM    <DIR>          ..\n",
      "09/16/2017  11:08 PM    <DIR>          .ipynb_checkpoints\n",
      "01/07/2017  12:22 PM    <DIR>          .metadata\n",
      "09/17/2017  12:53 PM            48,179 DeepLearningHelloWorld.ipynb\n",
      "01/08/2017  10:52 AM    <DIR>          Hello\n",
      "01/08/2017  10:52 AM    <DIR>          Hellocpp11\n",
      "01/09/2017  04:45 PM    <DIR>          HelloOpenCV\n",
      "09/17/2017  12:53 PM               723 model.json\n",
      "01/07/2017  12:22 PM    <DIR>          RemoteSystemsTempFiles\n",
      "               2 File(s)         48,902 bytes\n",
      "               8 Dir(s)  199,888,052,224 bytes free\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Optional] Step 11: Save the model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows\n",
      " Volume Serial Number is 7252-C405\n",
      "\n",
      " Directory of C:\\Users\\cobalt\\workspace\n",
      "\n",
      "09/17/2017  01:07 PM    <DIR>          .\n",
      "09/17/2017  01:07 PM    <DIR>          ..\n",
      "09/16/2017  11:08 PM    <DIR>          .ipynb_checkpoints\n",
      "01/07/2017  12:22 PM    <DIR>          .metadata\n",
      "09/17/2017  12:55 PM            47,588 DeepLearningHelloWorld.ipynb\n",
      "01/08/2017  10:52 AM    <DIR>          Hello\n",
      "01/08/2017  10:52 AM    <DIR>          Hellocpp11\n",
      "01/09/2017  04:45 PM    <DIR>          HelloOpenCV\n",
      "09/17/2017  01:07 PM            42,528 model.h5\n",
      "09/17/2017  12:53 PM               723 model.json\n",
      "01/07/2017  12:22 PM    <DIR>          RemoteSystemsTempFiles\n",
      "               3 File(s)         90,839 bytes\n",
      "               8 Dir(s)  199,868,473,344 bytes free\n"
     ]
    }
   ],
   "source": [
    "model.save_weights(\"model.h5\")\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Optional] Step 12: Load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.load_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Optional] Step 13: Compile and evaluate loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9440/10000 [===========================>..] - ETA: 0s\n",
      "Test score: 0.276437405369\n",
      "Test accuracy: 0.9228\n"
     ]
    }
   ],
   "source": [
    "loaded_model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=OPTIMIZER,\n",
    "              metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X_test, Y_test, verbose=VERBOSE)\n",
    "print(\"\\nTest score:\", score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- mkc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
